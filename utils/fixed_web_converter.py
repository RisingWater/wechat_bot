#!/usr/bin/env python3
import pypandoc
import requests
import os
import sys
import logging
from bs4 import BeautifulSoup
import tempfile
import re
import base64
import urllib.parse
from docx import Document

logger = logging.getLogger(__name__)

class FixedWebConverter:
    def __init__(self):
        self.session = requests.Session()
        self.setup_headers()
        self.base_url = None
    
    def setup_headers(self):
        """è®¾ç½®è¯·æ±‚å¤´"""
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        })
    
    def download_image(self, img_url):
        """ä¸‹è½½å›¾ç‰‡å¹¶è½¬æ¢ä¸ºbase64"""
        try:
            # å¤„ç†ç›¸å¯¹URL
            if not img_url.startswith('http'):
                if self.base_url:
                    img_url = urllib.parse.urljoin(self.base_url, img_url)
                else:
                    return None
            
            response = self.session.get(img_url, timeout=10)
            response.raise_for_status()
            
            # è½¬æ¢ä¸ºbase64
            image_data = base64.b64encode(response.content).decode('utf-8')
            
            # è·å–å›¾ç‰‡ç±»å‹
            content_type = response.headers.get('content-type', 'image/jpeg')
            if content_type == 'image/jpeg':
                data_uri = f"data:image/jpeg;base64,{image_data}"
            elif content_type == 'image/png':
                data_uri = f"data:image/png;base64,{image_data}"
            elif content_type == 'image/gif':
                data_uri = f"data:image/gif;base64,{image_data}"
            else:
                data_uri = f"data:{content_type};base64,{image_data}"
            
            return data_uri
            
        except Exception as e:
            logger.warning(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥ {img_url}: {e}")
            return None
    def fetch_and_clean_html(self, url):
        """è·å–å¹¶æ¸…ç†HTMLå†…å®¹ï¼Œå¤„ç†å›¾ç‰‡"""
        try:
            response = self.session.get(url)
            response.raise_for_status()
            self.base_url = url
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è·å–ç½‘é¡µæ ‡é¢˜ - ä¼˜å…ˆä½¿ç”¨ og:titleï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ title æ ‡ç­¾
            page_title = "æ— æ ‡é¢˜"
            
            # 1. é¦–å…ˆæ£€æŸ¥ og:title
            og_title = soup.find('meta', property='og:title')
            if og_title and og_title.get('content'):
                page_title = og_title['content'].strip()
            else:
                # 2. å¦‚æœæ²¡æœ‰ og:titleï¼Œä½¿ç”¨æ ‡å‡†çš„ title æ ‡ç­¾
                title_tag = soup.find('title')
                if title_tag and title_tag.get_text():
                    page_title = title_tag.get_text().strip()
            
            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                element.decompose()
    
            # å¤„ç†å›¾ç‰‡ - ä¸‹è½½å¹¶æ›¿æ¢ä¸ºbase64
            for img in soup.find_all('img'):
                img_src = img.get('src')
                if not img_src:
                    img_src = img.get('data-src')

                if img_src:
                    data_uri = self.download_image(img_src)
                    print(f"æ­£åœ¨å¤„ç†å›¾ç‰‡src: {img_src}")
                    if data_uri:
                        img['src'] = data_uri
                        logger.info(f"å·²åµŒå…¥å›¾ç‰‡: {img_src}")
                    else:
                        # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œä¿ç•™åŸå§‹é“¾æ¥
                        if not img_src.startswith('http') and self.base_url:
                            img['src'] = urllib.parse.urljoin(self.base_url, img_src)

            main_content = soup.find('main') or soup.find('article') or soup.find('body')
            cleaned_html = str(main_content)
            
            # è¿”å›ä¸¤ä¸ªå€¼ï¼šæ¸…ç†åçš„HTMLå’Œç½‘é¡µæ ‡é¢˜
            return cleaned_html, page_title
            
        except Exception as e:
            logger.error(f"è·å–æˆ–æ¸…ç†HTMLå¤±è´¥: {e}")
            raise
    
    def convert_html_to_docx(self, html_content, output_path):
        """ä½¿ç”¨pandocè½¬æ¢HTMLåˆ°DOCXï¼ˆä¿®å¤å‚æ•°ï¼‰"""
        
        # ä½¿ç”¨æ–°çš„å‚æ•°æ›¿ä»£ --self-contained
        extra_args = [
            '--standalone',
            '--embed-resources',      # æ›¿ä»£ --self-contained
            '--toc-depth=3',
        ]
        
        try:
            pypandoc.convert_text(
                source=html_content,
                to='docx',
                format='html',
                outputfile=output_path,
                extra_args=extra_args
            )
            
            logger.info(f"è½¬æ¢æˆåŠŸ: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"pandocè½¬æ¢å¤±è´¥: {e}")
            return False
    
    def paragraph_has_picture(self, paragraph):
        """è°ƒè¯•æ–‡æ¡£ç»“æ„ï¼ˆåŒ…å«å›¾ç‰‡æ£€æµ‹ï¼‰"""
        from docx.oxml.ns import qn

        for run in paragraph.runs:
            # æ–¹æ³•1ï¼šæ£€æŸ¥ drawing å…ƒç´ 
            drawings = run._element.findall('.//' + qn('w:drawing'))
            if drawings:
                print("paragraph.picture: w:drawing")
                return True
            
            # æ–¹æ³•2ï¼šæ£€æŸ¥ graphic å…ƒç´ 
            graphics = run._element.findall('.//' + qn('a:graphic'))
            if graphics:
                print("paragraph.picture: w:graphic")
                return True
            
            # æ–¹æ³•3ï¼šç›´æ¥æŸ¥æ‰¾ blip å…ƒç´ ï¼ˆå›¾ç‰‡å¼•ç”¨ï¼‰
            blips = run._element.findall('.//' + qn('a:blip'))
            if blips:
                print("paragraph.picture: w:blip")
                return True

        return False
    
    def remove_empty_paragraphs(self, docx_path):
        """ç§»é™¤DOCXæ–‡ä»¶ä¸­çš„ç©ºæ®µè½ï¼ˆéå¸¸å®‰å…¨ï¼Œç¡®ä¿ä¸åˆ é™¤ä»»ä½•æœ‰å†…å®¹çš„æ®µè½ï¼‰"""
        try:
            # æ‰“å¼€æ–‡æ¡£
            doc = Document(docx_path)
            
            # æ‰¾å‡ºæ‰€æœ‰ç©ºæ®µè½
            empty_paragraphs = []
            for paragraph in doc.paragraphs:
                # ä¸¥æ ¼æ£€æŸ¥ï¼šæ®µè½å¿…é¡»å®Œå…¨æ²¡æœ‰ä»»ä½•å†…å®¹
                # æ²¡æœ‰æ–‡æœ¬ å¹¶ä¸” æ²¡æœ‰runs æˆ–è€… æ‰€æœ‰runséƒ½æ˜¯ç©ºçš„
                text_empty = not paragraph.text.strip()
                # print("paragraph.text: ", paragraph.text.strip())
                runs_empty = not self.paragraph_has_picture(paragraph)
                # print("paragraph.picture: ", runs_empty)
                
                if text_empty and runs_empty:
                    empty_paragraphs.append(paragraph)
            
            # ç§»é™¤ç©ºæ®µè½ï¼ˆéœ€è¦åå‘éå†ï¼‰
            for paragraph in reversed(empty_paragraphs):
                p = paragraph._element
                p.getparent().remove(p)
            
            # ä¿å­˜æ–‡æ¡£
            doc.save(docx_path)
            logger.info(f"å·²ç§»é™¤ {len(empty_paragraphs)} ä¸ªç©ºæ®µè½")
            return True
            
        except Exception as e:
            logger.error(f"æ¸…ç†ç©ºæ®µè½å¤±è´¥: {e}")
            return False

    def convert_url_to_docx(self, url, output_dir):
        """ä¸»è½¬æ¢å‡½æ•°"""
        try:
            logger.info("æ­£åœ¨è·å–å’Œæ¸…ç†ç½‘é¡µå†…å®¹...")
            html_content, html_title = self.fetch_and_clean_html(url)

            safe_title = re.sub(r'[<>:"/\\|?*]', '', html_title)
            safe_title = safe_title.replace(' ', '_')  # ç©ºæ ¼æ›¿æ¢ä¸ºä¸‹åˆ’çº¿

            output_path = os.path.join(output_dir, f"{safe_title}.docx")
            
            logger.info("æ­£åœ¨è½¬æ¢ä¸ºDOCX...")
            success = self.convert_html_to_docx(html_content, output_path)
            if not success:
                return None
            
            self.remove_empty_paragraphs(output_path)
            return output_path
            
        except Exception as e:
            logger.error(f"è½¬æ¢è¿‡ç¨‹å¤±è´¥: {e}")
            return None
    
def main():
    import argparse
    
    parser = argparse.ArgumentParser(description='ä¿®å¤ç‰ˆç½‘é¡µè½¬DOCXå·¥å…·')
    parser.add_argument('url', help='ç½‘é¡µURL')
    parser.add_argument('-o', '--output', help='è¾“å‡ºDOCXæ–‡ä»¶è·¯å¾„', default='output.docx')
    
    args = parser.parse_args()
    
    converter = FixedWebConverter()
    success = converter.convert_url_to_docx(args.url, args.output)
    
    if success:
        logger.info(f"ğŸ‰ è½¬æ¢å®Œæˆï¼æ–‡ä»¶ä¿å­˜åœ¨: {os.path.abspath(args.output)}")
    else:
        logger.error("âŒ è½¬æ¢å¤±è´¥")
    
    sys.exit(0 if success else 1)

if __name__ == "__main__":
    main()